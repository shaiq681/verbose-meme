{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "---------\n",
    "\n",
    "The current file demostrates how to extract data from US SEC EDGAR filing reports. Post scraping, text analysis was performed to drive sentiment opinion, sentiment scores, readability, passive words, personal pronouns, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "------\n",
    "- **Positive Score**: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "- **Negative Score**: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n",
    "- **Polarity Score**: $\\frac{(\\text{Positive Score} â€“ \\text{Negative Score})}{((\\text{Positive Score + Negative Score} ) + 0.000001)}$\n",
    "- **Subjective Score**: $\\frac{\\text{(Positive Score + Negative Score)}}{\\text{((Total Words after cleaning) + 0.000001)}}$\n",
    "- **Average Sentece Length**: $\\frac{\\text{No. of Words}}{\\text{Np. of Sentences}}$\n",
    "- **% Complex Words**: $\\frac{\\text{The number of complex words}}{\\text{The number of words }}$\n",
    "- **Fog Index**: $0.4 * \\text{(Average Sentence Length + % Complex words)}$\n",
    "- **Syllable Count**: Personal Pronouns are considered for counting. Special care is taken so that the country name US is not included in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  sys, os, glob\n",
    "import  re\n",
    "import  numpy as np\n",
    "import  pandas as pd\n",
    "import  bs4 as bs\n",
    "import  requests \n",
    "import  spacy # NLP\n",
    "from    spacy_syllables import SpacySyllables # Syllables\n",
    "from    pathlib     import Path\n",
    "from    string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_syllables.SpacySyllables at 0x20bb351fc88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # Custom Model\n",
    "\n",
    "nlp.add_pipe('syllables', after='tagger') # Model for Syllable identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_LIMIT = 1_000_000 # Default SpaCy limit\n",
    "\n",
    "URLArchive = \"https://www.sec.gov/Archives/\"\n",
    "\n",
    "CONSTANT = 0.000_001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Factory\n",
    "\n",
    "- Create List of the following Constants:\n",
    "    - Positive Words \n",
    "    - Negative Words\n",
    "    - Stop Words(_Auditiors_, _Currencies_,_Dates and Numbers_,_Generic_, _Generic Long_, _Geographic_, _Names_)\n",
    "    - Constraining Words\n",
    "    - Uncertainty Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords\n",
    "\n",
    "- Scraped Stopwords from the [Official Source](https://sraf.nd.edu/textual-analysis/resources/ )\n",
    "- Saved Words in the folder _SentinemtWordList_\n",
    "- Collected __positive__ and __negative__ words for creating a StopWords Corpus.\n",
    "- Used Similar approach to create a corpus of __constraint__ and __uncertainty__ words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = sorted(Path('.').glob(f\"**/*SentimentWordList*\")) # Stop Words file Path\n",
    "sentiment_file = Path.cwd()/Path(path[0])\n",
    "negs = pd.read_excel(io=sentiment_file, sheet_name='Negative',header=None).iloc[:,0].values\n",
    "pos = pd.read_excel(io=sentiment_file, sheet_name='Positive',header=None).iloc[:,0].values\n",
    "\n",
    "neg_words = [word.lower() for word in negs]\n",
    "pos_words = [word.lower() for word in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createStopWords():\n",
    "    \"\"\"Function to fetch Stopwords from various stop-words text files as provided in reference.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    pathStopWords = sorted(Path('.').glob(f\"**/stopwords*.txt\")) # Stop Words file Path\n",
    "    if pathStopWords:\n",
    "        for filePath in pathStopWords:\n",
    "            fullPath =  Path.cwd()/Path(filePath)\n",
    "            with open(fullPath, mode='r', encoding='utf8', errors='ignore') as file:\n",
    "                TEXT = file.read()\n",
    "                fileStopWOrds = [line.split('|')[0] for line in TEXT.split('\\n')]\n",
    "                stop_words.extend(fileStopWOrds)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'StopWords related files are not in the {Path.cwd()} and its subdirectories')\n",
    "    return(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopWordList = createStopWords() # Collect Stop words from all files\n",
    "stopWordList = list(map(lambda x: x.lower(), stopWordList)) \n",
    "nlp.Defaults.stop_words -= nlp.Defaults.stop_words # Remove Default Stop Words\n",
    "nlp.Defaults.stop_words = {stopword for stopword in stopWordList if stopword !=\"\"} # Add custom stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Constraint list\n",
    "path_constraint = sorted(Path('.').glob(f\"**/*constraining*\")) # Stop Words file Path\n",
    "constraining_file = Path.cwd()/Path(path_constraint[0])\n",
    "constraints = pd.read_excel(io=constraining_file, sheet_name=0,header=0).iloc[:,0].values\n",
    "constraints = [w.lower() for w in constraints]\n",
    "# Create Uncertainty List\n",
    "path_uncertainty = sorted(Path('.').glob(f\"**/*uncertainty*\")) # Stop Words file Path\n",
    "uncertainty_file = Path.cwd()/Path(path_uncertainty[0])\n",
    "uncertainty = pd.read_excel(io=uncertainty_file, sheet_name=0,header=0).iloc[:,0].values\n",
    "uncertainty = [w.lower() for w in uncertainty]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "- Performs Data cleaning, formating and NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceHTMLTags(text):\n",
    "    \"Function that uses Regex to create word tokens.\"\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text) # Remove HTML Tags\n",
    "    text = re.sub(r'[^\\w\\s]',\"\", text) # Remove Punctuations\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data Scraping\n",
    "\n",
    "def save_text(url):\n",
    "    \"\"\"Function to save the results of the scraped text in 'raw' directory.\"\"\"\n",
    "    file_name =  Path.cwd()/Path(\"raw/\"+url.split('/')[-1]) # Save .txt file in raw folder\n",
    "    try:\n",
    "        data  = requests.get(url, timeout = 10) # Standard Timeout according to SEC.gov\n",
    "        response = data.status_code\n",
    "        if response >200:\n",
    "            return(url)\n",
    "        else:\n",
    "            data = data.content.decode('utf-8')\n",
    "            with open(file_name,  'w') as f:\n",
    "                f.write(data)\n",
    "    except:\n",
    "        pass\n",
    "        # raise ProxyError('Unable to connect!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_txt(secfname):\n",
    "    \"\"\"\n",
    "    Function to read from the text file related to SECFNAME.\n",
    "    Params:\n",
    "    ------\n",
    "    secfname: str, SECFNAME column value.\n",
    "    \"\"\"\n",
    "    text_file = secfname.split('/')[-1]\n",
    "    file_path = sorted(Path('.').glob(f\"**/{text_file}\"))\n",
    "    with open(file_path[0]) as f:\n",
    "        TEXT = f.read()\n",
    "    return(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(TEXT):\n",
    "    \"\"\"Function to get various count in a text.\n",
    "    Params:\n",
    "    ------\n",
    "    TEXT: str, Input text\n",
    "    Returns:\n",
    "    -------\n",
    "    List[count_pos_sents,count_neg_sents, total_complex_words, \n",
    "        total_words, total_sents, total_syllables, total_const, total_uncertain]:\n",
    "\n",
    "        count_pos_sents: int, No. of Positive words in the SEC Filing\n",
    "        count_neg_sents: int, No. of Negative words in the SEC Filing\n",
    "        total_complex_words: int, No of Complex words in the SEC Filing\n",
    "        total_words: int, Total words post cleanup in the SEC Filing\n",
    "        total_sents :  int, Total sentences in the SEC Filing\n",
    "        total_syllables: int, Total No of complex Syllables in the SEC Filing\n",
    "        total_const: int, Total No of Constraint words\n",
    "        total_uncertain: int, Total no. of Uncertain words\n",
    "    \"\"\"\n",
    "    print(\"startint sentiment analysis...\")\n",
    "    if not TEXT:\n",
    "        return([CONSTANT]*7) # To avoid ZeroDivisionError\n",
    "    else:\n",
    "        if len(TEXT)< TEXT_LIMIT:\n",
    "            doc  = nlp(TEXT)\n",
    "        else:\n",
    "            nlp.max_length = len(TEXT) +1\n",
    "            doc  =  nlp(TEXT, disable = ['ner'])\n",
    "            print(\"document loaded...\")\n",
    "        \n",
    "        count_pos_sents = 0\n",
    "        count_neg_sents = 0\n",
    "        total_complex_words = 0\n",
    "        total_words = 0\n",
    "        total_const = 0\n",
    "        total_uncertain = 0\n",
    "        total_sents = 0\n",
    "        for token in doc:\n",
    "            # Positive Word Count\n",
    "            if (token.lower_ in pos_words):\n",
    "                count_pos_sents += 1\n",
    "            # Negative Word Count\n",
    "            if (token.lower_ in neg_words):\n",
    "                count_neg_sents +=1\n",
    "            # Complex Word Count\n",
    "            if (token._.syllables_count is not None and token._.syllables_count >2):\n",
    "                total_complex_words +=1\n",
    "            # Total Words\n",
    "            if (token.lower_ not in nlp.Defaults.stop_words):\n",
    "                total_words +=1\n",
    "            # Count Constraints\n",
    "            if (token.lower_ in constraints):\n",
    "                total_const +=1\n",
    "            # Count uncertainty\n",
    "            if (token.lower_ in uncertainty):\n",
    "                total_uncertain +=1\n",
    "        # Total Sentences\n",
    "        total_sents = sum(1 for sent in doc.sents)\n",
    "        return([count_pos_sents,count_neg_sents, total_complex_words, \n",
    "        total_words, total_sents,  total_const, total_uncertain])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_excel('cik_list.xlsx', sheet_name='cik_list_ajay', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining Pipeline\n",
    "---------    \n",
    "- Reads the TEXT file into the SpaCy's vectorized format.\n",
    "- Removes possible HTML related tags and punctuations.\n",
    "- Performs Sentiment analysis: calculates, positive, negative and polarity score\n",
    "- Calculates Complex Word count\n",
    "- Calculates Total Word count\n",
    "- Calculates Uncertainty count\n",
    "- Calculates Constraining count.\n",
    "- Retuns results as pandas columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mining_pipeline = lambda secfname: get_sentiments(replaceHTMLTags(read_from_txt(secfname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "document loaded...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n",
      "startint sentiment analysis...\n"
     ]
    }
   ],
   "source": [
    "# Calculated Positivity Score, Negativity Score, Complex WOrd Count, Word Count, Sentence Length,\n",
    "# Uncertainity Score and Constraining Score\n",
    "data[['positive_score','negative_score','complex_word_count','word_count','sentence_length',\\\n",
    "    'uncertainty_score', 'constraining_score']] = data.apply(lambda x: text_mining_pipeline(x['SECFNAME']), axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Polarity Score\n",
    "data['polarity_score'] = data.apply(lambda row: (row['positive_score'] - row['negative_score'])/(row['positive_score'] + row['negative_score']+ CONSTANT) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Average Sentence Length\n",
    "data['average_sentence_length'] = data.apply(lambda row: row['word_count']/row['sentence_length'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated % Complex Words\n",
    "data['percentage_of_complex_words'] = data.apply\\\n",
    "    (lambda row: (row['complex_word_count']/row['word_count'])*100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Fog Index\n",
    "data['fog_index'] = 0.4*(data['average_sentence_length'] + data['percentage_of_complex_words']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Positive Word Proportion\n",
    "data['positive_word_proportion'] = data.apply(lambda r: r['positive_score']/r['word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Negative Word Proportion\n",
    "data['negative_word_proportion'] = data.apply(lambda r: r['negative_score']/r['word_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Uncertainty Word Proportion\n",
    "data['uncertainty_word_proportion'] = data.apply(lambda r: r['uncertainty_score']/ r['word_count'] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Constraining Word Proportion\n",
    "data['constraining_word_proportion'] = data.apply(lambda r: r['constraining_score']/ r['word_count'] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated Constrainig Word in whole Report\n",
    "data['constraining_words_whole_report'] = np.sum(data.constraining_score) # Broadcasting the total constraining score of all the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path_or_buf=Path.cwd()/Path('result.csv'),\n",
    "columns=['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'SECFNAME', 'positive_score', \\\n",
    "    'negative_score', 'polarity_score', 'average_sentence_length', 'percentage_of_complex_words', 'fog_index',\\\n",
    "        'complex_word_count', 'word_count', 'uncertainty_score', 'constraining_score', 'positive_word_proportion',\\\n",
    "            'negative_word_proportion', 'uncertainty_word_proportion', 'constraining_word_proportion', 'constraining_words_whole_report'],\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
